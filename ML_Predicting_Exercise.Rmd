---
title: "Predicting Exercise from Activity Data"
#output: pdf_document
output: html_document
---

###Executive Summary

This report was created for the Practical Machine Learning course offered by Johns Hopkins University on the Coursera platform.

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. In this project, the goal was to work with data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. The participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The objective is to predict the manner in which they did the exercise.

The following sections describe the analysis and cleaning of the data, along with the development of the prediction model used. This model correctly predicted the results for all of the test data observations on the first attempt.

###Exploratory Data Analysis

Initial analysis of the testing data showed a large number of variables that did not appear to contain valid measurement values.

Read Training Data:
```{r }

pmlData=read.csv("pml-training.csv",stringsAsFactors=FALSE,header=TRUE,na.strings=c("","#DIV/0!",NA,"NA"))
pmlData$classe <- as.factor(pmlData$classe)

```

Remove NA Columns:
```{r }
lst <- colSums(is.na(pmlData)); lst <- sapply(lst, function(x)all(sum(x<19000))); mynames <- names(which(lst));
pmlData <- subset(pmlData,select = names(pmlData) %in% mynames)
```

Remove unhelpful columns: timestamp, etc.:
```{r }
`%ni%` <- Negate(`%in%`);
xCols <- c("X","user_name","raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp","new_window","num_window");
pmlData <- subset(pmlData,select = names(pmlData) %ni% xCols)
```

These steps resulted in 53 columns, including the classe variable.

###Model Development

The initial assumption was that we still had too many variables to work with, resulting in the decision to do a PCA. This was in addition to wanting to do cross-validation.

To do cross-validation, four partitions were created from the testing data using the following pattern, using 75% as training data.

```{r message=FALSE}
library(caret)
part1 <- createDataPartition(pmlData$classe, p=.75, list=F)
train1 <- pmlData[part1,]
test1 <- pmlData[-part1,]
```

Then, caret's train function with the "rf" method was used as the initial model attempt. The time involved in processing 75 percent of the first test data partition proved onerous given the available processing resources, so a partition was created using 10% of the data for training. The response was obviously quicker, however surprisingly the confusion matrix gave an accuracy level in the low 80s. I raised the test partition to 25%, which took longer, as expected, but still surprisingly had an accuracy only in the low 90% range. With the performance and accuracy observed, the decision was made to attempt another type of mode.

```{r results='hide', message=FALSE}
#commented out for report generation
#preProc1<-preProcess(train1[,-53],method="pca",thresh=.8)
#trainPC1 <- predict(preProc1, train1[,-53])
#mFit1.rf <- train(train1$classe ~ ., method = "rf", data = trainPC1, prox=TRUE)
#testPC1 <- predict(preProc1, test1[,-53])

#pred1.rf <- predict(mFit1.rf,testPC1)
#confusionMatrix(test1$classe, pred1.rf)

```

Still using caret's train function, but using the "gbm" method, things improved somewhat. Using a 75/25 partitioning, without PCA, resulted in a 96% accuracy. This is definitely a good result, but running this model against the final test data would likely result in at least one incorrect error. With a goal of 100% accuracy on the test set, I continued to search for a promising model.

```{r results='hide', message=FALSE}
#commented out for report generation
#gbmFit2 <- train(classe ~ ., method = "gbm", data = train1)
#pred.gbm2 <- predict(gbmFit2,test1)
#confusionMatrix(test1$classe, pred.gbm2)
```

The next attempt was with the randomForest() function. Once again using a 75/25 partitioning of the training data, this model had an accuracy of 99.59%. It was also clearly superior in terms of performance.

```{r results='hide', message=FALSE}
#commented out for report generation
#library(randomForest)
#rf <- randomForest(classe ~ ., data=train1,ntree=100,mtry=3,importance=TRUE)
#pred <- predict(rf,test1)
#confusionMatrix(test1$classe,pred)

```

###Results

The results generated at test data set predictions (B, A, B, A, A, E, D, B, A, A, B, C, B, A, E, E, A, B, B, B) were submitted and correctly predicted 20/20 results on the first pass.
